name: Clean, Validate, and Upload Customer Data

on:
  workflow_dispatch: # Möjliggör manuell körning av workflow

jobs:
  clean-data:
    runs-on: ubuntu-latest

    steps:
      # Check out repository
      - name: Check out repository
        uses: actions/checkout@v3

      # Create directories if not existing
      - name: Prepare directories
        run: |
          mkdir -p cleaned_data
          mkdir -p validate_data

      # Set up Python environment
      - name: Set up Python
        uses: actions/setup-python@v3
        with:
          python-version: '3.9'

      # Install dependencies
      - name: Install dependencies
        run: pip install -r requirements.txt

      # Run clean-up script
      - name: Run clean-up script
        run: python pipeline_scripts/clean_up.py

      # Verify cleaned_data files
      - name: Verify cleaned_data files
        run: ls cleaned_data/

      # Verify validate_data files
      - name: Verify validate_data files
        run: ls validate_data/

      # Upload cleaned data artifact
      - name: Upload cleaned data artifact
        uses: actions/upload-artifact@v3
        with:
          name: cleaned-data
          path: cleaned_data/

      # Upload dirty data artifact
      - name: Upload dirty data artifact
        uses: actions/upload-artifact@v3
        with:
          name: validate-data
          path: validate_data/

  validate-data:
    needs: clean-data
    runs-on: ubuntu-latest

    steps:
      # Check out repository
      - name: Check out repository
        uses: actions/checkout@v3

      # Download dirty data artifact
      - name: Download dirty data artifact
        uses: actions/download-artifact@v3
        with:
          name: validate-data

      # Set up Python environment
      - name: Set up Python
        uses: actions/setup-python@v3
        with:
          python-version: '3.9'

      # Install dependencies
      - name: Install dependencies
        run: pip install -r requirements.txt

      # Run validation script
      - name: Run validation script
        run: python pipeline_scripts/validate_data.py validate_data/errors.xlsx

  backup-and-upload:
    needs: validate-data
    runs-on: ubuntu-latest

    steps:
      # Check out repository
      - name: Check out repository
        uses: actions/checkout@v3

      # Set up Python environment
      - name: Set up Python
        uses: actions/setup-python@v3
        with:
          python-version: '3.9'

      # Install dependencies
      - name: Install dependencies
        run: pip install -r requirements.txt

      # Download cleaned data artifact
      - name: Download cleaned data artifact
        uses: actions/download-artifact@v3
        with:
          name: cleaned-data

      # Convert cleaned data to CSV
      - name: Convert cleaned data to CSV
        run: python pipeline_scripts/convert_to_csv.py cleaned_data/clean_data.xlsx cleaned_data/clean_data.csv

      # Backup cleaned data to Azure Blob Storage
      - name: Backup cleaned data to Azure Blob Storage
        env:
          AZURE_STORAGE_ACCOUNT: ${{ secrets.AZURE_STORAGE_ACCOUNT }}
          AZURE_STORAGE_KEY: ${{ secrets.AZURE_STORAGE_KEY }}
        run: |
          az storage blob upload \
            --account-name $AZURE_STORAGE_ACCOUNT \
            --account-key $AZURE_STORAGE_KEY \
            --container-name cleaned-data-backup \
            --file cleaned_data/clean_data.csv \
            --name clean_data_backup_$(date +'%Y%m%d%H%M%S').csv

  test-db-connection-and-upload:
    needs: backup-and-upload
    runs-on: ubuntu-latest

    steps:
      # Check out repository
      - name: Check out repository
        uses: actions/checkout@v3

      # Set up Python environment
      - name: Set up Python
        uses: actions/setup-python@v3
        with:
          python-version: '3.9'

      # Install dependencies
      - name: Install dependencies
        run: pip install -r requirements.txt

      # Download cleaned data artifact
      - name: Download cleaned data artifact
        uses: actions/download-artifact@v3
        with:
          name: cleaned-data

      # Test database connection
      - name: Test database connection
        env:
          SQL_SERVER: ${{ secrets.SQL_SERVER }}
          SQL_DATABASE: ${{ secrets.SQL_DATABASE }}
          SQL_USER: ${{ secrets.SQL_USER }}
          SQL_PASSWORD: ${{ secrets.SQL_PASSWORD }}
        run: python pipeline_scripts/test_db_connection.py

      # Upload clean data to SQL database
      - name: Upload clean data to SQL database
        env:
          SQL_SERVER: ${{ secrets.SQL_SERVER }}
          SQL_DATABASE: ${{ secrets.SQL_DATABASE }}
          SQL_USER: ${{ secrets.SQL_USER }}
          SQL_PASSWORD: ${{ secrets.SQL_PASSWORD }}
        run: python pipeline_scripts/upload_to_sql.py cleaned_data/clean_data.xlsx
